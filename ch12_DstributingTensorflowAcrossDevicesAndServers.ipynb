{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Qm8Nbb17CBW",
    "colab_type": "text"
   },
   "source": [
    "# Chapter 12: Distributing Tensorflow Across Devices and Servers\n",
    "----\n",
    "----\n",
    "\n",
    "Tensorflow's supportof  distributed computing is one of its main highlights. Full control of how to split your computation graph across servers and devices.\n",
    "\n",
    "## Multiple Devices on a Single Machine\n",
    "----\n",
    "\n",
    "#### Installation\n",
    "\n",
    "- [AWS instunctions](http://goo.gl/kbge5b)\n",
    "- [Google Cloud Learning](https://cloud.google.com/ml)\n",
    "- [Good build options for deep learning](https://goo.gl/pCtSAn)\n",
    "- Currently using Colab\n",
    "\n",
    "Steps: [GPU Tensorflow](https://www.tensorflow.org/install/gpu)\n",
    "- Insall Nvidia Drivers\n",
    "- Install CUDA toolkit\n",
    "- Install cuDNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pM9IaRpC5LAk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CUaOi8v267kI",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "3fd6367e-209a-4088-b57d-c7defcedc057",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.539862204066E12,
     "user_tz": 240.0,
     "elapsed": 1220.0,
     "user": {
      "displayName": "Thomas Chapados-Muermans",
      "photoUrl": "",
      "userId": "17350228220160903532"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XV7BjOSODiTd",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312.0
    },
    "outputId": "211210bd-18bd-4b58-d1e1-ab0894e09f99",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.539864147444E12,
     "user_tz": 240.0,
     "elapsed": 600.0,
     "user": {
      "displayName": "Thomas Chapados-Muermans",
      "photoUrl": "",
      "userId": "17350228220160903532"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6249105289000007423\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11281553818\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6822170813712466320\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-WfMLuTI2Xj",
    "colab_type": "text"
   },
   "source": [
    "#### Managin the GPU RAM\n",
    "\n",
    "Default, Tensorflow uses all of the RAM available, the first time the graph is ran.\n",
    "\n",
    "\n",
    "Set programs on different GPUs\n",
    "```\n",
    "$ CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n",
    "# and in another terminal\n",
    "$ CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\n",
    "```\n",
    "\n",
    "Or, tell Tensorflow to use part of the memory:\n",
    "\n",
    "```(python)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "```\n",
    "\n",
    "Or, grab memory only when needed:\n",
    "```(python)\n",
    "config.gpu_options.allow_growth = True\n",
    "```\n",
    "\n",
    "#### Placing Operations on Devices\n",
    "[The Tensorflow whitepaper](http://goo.gl/vSjA14)\n",
    "\n",
    "##### Simple placement\n",
    "Rules:\n",
    "- If a node was placed on a device in a pervious run of the graph, it is left on that device\n",
    "- Else, if the user pinned a node to a device, the placer places it on that device\n",
    "- Else, it defaults to GPU#0, or CPU if there is not GPU\n",
    "\n",
    "```(python)\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.Variable(4.0)\n",
    "c = a * b\n",
    "```\n",
    "\n",
    "a and b are pinned to cpu:0 and c is not pinned so it will default to gpu:0\n",
    "\n",
    "##### Logging placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "g4Rh00SFMwqk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP_eNYh6ObNQ",
    "colab_type": "text"
   },
   "source": [
    "##### Dynamic placement function\n",
    "\n",
    "place variables on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "qtezPGd7OogQ",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def variables_on_cpu(op):\n",
    "  if op.type == \"Variable\":\n",
    "    return \"/cpu:0\"\n",
    "  else:\n",
    "    return \"/gpu:0\"\n",
    "  \n",
    "with tf.device(variables_on_cpu):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.Variable(4.0)\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCKfDxajPLax",
    "colab_type": "text"
   },
   "source": [
    "##### Operations and kernels\n",
    "For a Tensorflow operation to run on a device, it needs to have an implementation for that device; this is called a kernel.\n",
    "\n",
    "NB: Variables do not have support for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A58OUryfP9JP",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3276.0
    },
    "outputId": "6dfc2984-51e2-4dc2-ac2f-099e23a18549",
    "executionInfo": {
     "status": "error",
     "timestamp": 1.539867559271E12,
     "user_tz": 240.0,
     "elapsed": 1231.0,
     "user": {
      "displayName": "Thomas Chapados-Muermans",
      "photoUrl": "",
      "userId": "17350228220160903532"
     }
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nIdentity: GPU CPU \nVariableV2: CPU \nAssign: CPU \n\nColocation members and user-requested devices:\n  Variable (VariableV2) /device:GPU:0\n  Variable/Assign (Assign) /device:GPU:0\n  Variable/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n  device='CPU'\n\n\t [[{{node Variable}} = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d4daa87257e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m           self._config.experimental.client_handles_error_formatting):\n\u001b[1;32m   1307\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nIdentity: GPU CPU \nVariableV2: CPU \nAssign: CPU \n\nColocation members and user-requested devices:\n  Variable (VariableV2) /device:GPU:0\n  Variable/Assign (Assign) /device:GPU:0\n  Variable/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n  device='CPU'\n\n\t [[{{node Variable}} = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n\nCaused by op 'Variable', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-7d4daa87257e>\", line 4, in <module>\n    i = tf.Variable(3)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 145, in __call__\n    return cls._variable_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 141, in _variable_call\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 120, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\", line 2441, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 147, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1104, in __init__\n    constraint=constraint)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\", line 1240, in _init_from_args\n    name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\", line 77, in variable_op_v2\n    shared_name=shared_name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 1357, in variable_v2\n    shared_name=shared_name, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nColocation Debug Info:\nColocation group had the following types and devices: \nIdentity: GPU CPU \nVariableV2: CPU \nAssign: CPU \n\nColocation members and user-requested devices:\n  Variable (VariableV2) /device:GPU:0\n  Variable/Assign (Assign) /device:GPU:0\n  Variable/read (Identity) /device:GPU:0\n\nRegistered kernels:\n  device='GPU'; dtype in [DT_INT64]\n  device='GPU'; dtype in [DT_DOUBLE]\n  device='GPU'; dtype in [DT_FLOAT]\n  device='GPU'; dtype in [DT_HALF]\n  device='CPU'\n\n\t [[{{node Variable}} = VariableV2[container=\"\", dtype=DT_INT32, shape=[], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "reset_graph()\n",
    "sess = tf.Session()\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  i = tf.Variable(3)\n",
    "sess.run(i.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wb6bqMLQqID",
    "colab_type": "text"
   },
   "source": [
    "##### Soft placement\n",
    "Fall back to cpu set **allow_soft_placement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "JJ7_jGzcRBTy",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "  i = tf.Variable(3.0)\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(i.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AelanNiRcl7",
    "colab_type": "text"
   },
   "source": [
    "#### Parallel Execution\n",
    "\n",
    "can control # of thread per inter-op pool\n",
    " - `inter_op_parallelism_threads`\n",
    " - `use_per_session_threads`\n",
    " - `intra_op_parallelism_threads`\n",
    "\n",
    "#### Control Dependencies\n",
    "to postpone evaluation of some node, simple soltuion is to add _control dependencies_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "zElYCcFZS_CF",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.constant(1.0)\n",
    "b = a + 2.0\n",
    "\n",
    "with tf.control_dependencies([a,b]):\n",
    "  x = tf.constant(3.0)\n",
    "  y = tf.constant(4.0)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gydIj8w8TbrJ",
    "colab_type": "text"
   },
   "source": [
    "## Multiple Devices Across Multiple Servers\n",
    "----\n",
    "First define a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "GZkKRyQVT_UD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        \"127.0.0.1:2221\",  # /job:ps/task:0\n",
    "        \"127.0.0.1:2222\",  # /job:ps/task:1\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        \"127.0.0.1:2223\",  # /job:worker/task:0\n",
    "        \"127.0.0.1:2224\",  # /job:worker/task:1\n",
    "        \"127.0.0.1:2225\",  # /job:worker/task:2\n",
    "    ]})\n",
    "\n",
    "task_ps0 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=0)\n",
    "task_ps1 = tf.train.Server(cluster_spec, job_name=\"ps\", task_index=1)\n",
    "task_worker0 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)\n",
    "task_worker1 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=1)\n",
    "task_worker2 = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w8j--R8UglG",
    "colab_type": "text"
   },
   "source": [
    "#### Opening a Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EAAGG9GeUtub",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "a = tf.constant(1.0)\n",
    "b = a + 2\n",
    "c = a * 2\n",
    "\n",
    "with tf.Session(\"grpc://machine-b.example.com:2222\") as sess:\n",
    "  print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H70qLS2VBZk",
    "colab_type": "text"
   },
   "source": [
    "#### The Master and Worker Services\n",
    "\n",
    "The client uses the Google Remote Procedure Call to communicate with the server.\n",
    "\n",
    "#### Pinning Operatings Across Tasks\n",
    "You can use device blocks to pin operations on any device managed by any task, by specifying the job name, task index, device type and device index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "3enqx_yaWFgD",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "with tf.device(\"/job:ps/task:0/cpu:0\"):\n",
    "  # ...\n",
    "\n",
    "with tf.device(\"/job:worker/task:0/gpu:1\"):\n",
    "  # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud6tvE-AWYsb",
    "colab_type": "text"
   },
   "source": [
    "#### Sharding Variables Across Multiple Parameter Servers\n",
    "\n",
    "Store model parameters on a set of parameter servers(ps job) while other tasks focus on computations(worker jobs)\n",
    "\n",
    "`replica_device_setter()` distributes variables across all the \"ps\" tasks in a round robin fashion\n",
    "\n",
    "#### Sharing State Across Sessions Using Resource Containers\n",
    "\n",
    "variable state is managed by resource containers located on the cluster itself, not by the session.\n",
    "\n",
    "#### Asynchronous Communications Using Tensorflow Queues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "mMHIbdHgY7aC",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]], name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "training_instance = tf.placeholder(tf.float32, shape=(2))\n",
    "enqueue = q.enqueue([training_instance])\n",
    "\n",
    "with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "  sess.run(enqueue, feed_dict={training_instance:[1., 2.]})\n",
    "  # ...\n",
    "  \n",
    "  \n",
    "dequeue = q.dequeue()\n",
    "\n",
    "with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "  print(sess.run(dequeue)) # [1., 2.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlVp3ROpaJY0",
    "colab_type": "text"
   },
   "source": [
    "#### Loading Data Directly From the Graph\n",
    "\n",
    "\n",
    "\n",
    "## Paralelizing Neural Networks on a Tensorflow Cluster\n",
    "----\n",
    "First we look at how to parallelize several neural nets by placing each one on different device. The we look at training single network across multiple devices and servers.\n",
    "\n",
    "#### One Neural Net per Device\n",
    "\n",
    "specify master server address when creating a session\n",
    "\n",
    "\n",
    "#### Model Parallelism\n",
    "depends on architecture of neural net, it can be tricky."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ch12_DstributingTensorflowAcrossDevicesAndServers.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
